{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "038315fa-9917-4139-b173-d45fa0e6e9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web scrapping started...one moment please.\n",
      "Start time: 20:13:57\n",
      "Starting meet processing.\n",
      "Girls State 1\n",
      "End time:   20:18:03\n",
      "Elapsed time: 0:04:05.808249\n",
      "2025 Girls State Results.csv and 2025 Girls State Warnings.csv created.\n",
      "Web scrapping completed.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# CONFIGURATION - Must update this section with values you want.\n",
    "# -------------------------------------------------------------\n",
    "DB_PATH = \"db/Track.db\"\n",
    "YEAR = 2025  # valid options: 2023, 2024, 2025\n",
    "GENDER = \"Girls\"  # valid options: Girls, Boys\n",
    "MEET_TYPE_LIST = [\"State\"]  # valid options: Sectional, Regional, SFtate\n",
    "SEC_TO_PROCESS = list(range(1, 33))  # Sectionals to process: 1, 33 for all\n",
    "REG_TO_PROCESS = list(range(1, 9))  # Regionals to process: 1, 9 for all\n",
    "\n",
    "# TFRRS URL suffix based on year\n",
    "if YEAR == 2025:\n",
    "    tfrrs_url_suffix = \"?config_hnd=395\"\n",
    "elif YEAR == 2024:\n",
    "    tfrrs_url_suffix = \"?config_hnd=342\"\n",
    "elif YEAR == 2023:\n",
    "    tfrrs_url_suffix = \"?config_hnd=299\"\n",
    "else:\n",
    "    tfrrs_url_suffix = \"\"\n",
    "    \n",
    "# -------------------------------------------------------------\n",
    "# IMPORTS\n",
    "# -------------------------------------------------------------\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "import gc\n",
    "import sys\n",
    "import random\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "from util.db_util import Database\n",
    "from util.conversion_util import Conversion\n",
    "from datetime import datetime\n",
    "from difflib import SequenceMatcher\n",
    "from sqlite3 import IntegrityError\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# GLOBALS INIT\n",
    "# -------------------------------------------------------------\n",
    "session = requests.Session()\n",
    "warningDF = pd.DataFrame(columns=[\"warning\", \"id\", \"desc\"])\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# WEB SCRAPER START\n",
    "# -------------------------------------------------------------\n",
    "print(\"Web scrapping started...one moment please.\")\n",
    "start_time = datetime.now()\n",
    "print(f\"Start time: {start_time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# USER AGENTS\n",
    "# -------------------------------------------------------------\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    \"(KHTML, like Gecko) Chrome/116.0.5845.140 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5) AppleWebKit/605.1.15 \"\n",
    "    \"(KHTML, like Gecko) Version/17.6 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "    \"(KHTML, like Gecko) Firefox/140.0\",\n",
    "]\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# WARNINGS\n",
    "# -------------------------------------------------------------\n",
    "W_ATHLETE_NOT_FOUND = \"Athlete not found.\"\n",
    "W_TEAM_NOT_FOUND = \"Team not found.\"\n",
    "W_ROSTER_NOT_FOUND = \"Roster not found.\"\n",
    "W_RELAY_ATHLETES_NOT_FOUND = \"Relay athletes not found.\"\n",
    "W_GRADE_LEVEL_NOT_FOUND = \"Grade level not found.\"\n",
    "W_ATTEMPT_FAILED = \"Attempt failed for URL.\" \n",
    "W_DUPLICATE_ATHLETE = \"Duplicate athlete.\" \n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# EVENTS LIST\n",
    "# -------------------------------------------------------------\n",
    "events = []\n",
    "convert = Conversion()\n",
    "\n",
    "if GENDER == \"Girls\":\n",
    "    events += [(GENDER, \"Prelim\", \"100 Hurdles\"), (GENDER, \"Track\", \"100 Hurdles\")]\n",
    "else:\n",
    "    events += [(GENDER, \"Prelim\", \"110 Hurdles\"), (GENDER, \"Track\", \"110 Hurdles\")]\n",
    "\n",
    "events += [\n",
    "    (GENDER, \"Prelim\", \"100 Meters\"), (GENDER, \"Prelim\", \"200 Meters\"),\n",
    "    (GENDER, \"Track\", \"100 Meters\"), (GENDER, \"Track\", \"200 Meters\"),\n",
    "    (GENDER, \"Track\", \"400 Meters\"), (GENDER, \"Track\", \"800 Meters\"),\n",
    "    (GENDER, \"Track\", \"1600 Meters\"), (GENDER, \"Track\", \"3200 Meters\"),\n",
    "    (GENDER, \"Track\", \"300 Hurdles\"), (GENDER, \"Relay\", \"4 x 100 Relay\"),\n",
    "    (GENDER, \"Relay\", \"4 x 400 Relay\"), (GENDER, \"Relay\", \"4 x 800 Relay\"),\n",
    "    (GENDER, \"Field\", \"High Jump\"), (GENDER, \"Field\", \"Long Jump\"),\n",
    "    (GENDER, \"Field\", \"Discus\"), (GENDER, \"Field\", \"Shot Put\"),\n",
    "    (GENDER, \"Field\", \"Pole Vault\")\n",
    "]\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# CREATE CACHES\n",
    "# -------------------------------------------------------------\n",
    "team_cache = {}\n",
    "athlete_url_cache = {}\n",
    "athlete_db_cache = {}\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# PROCESS WARNING FUNCTION\n",
    "# -------------------------------------------------------------\n",
    "def log_warning(warning, id, desc):\n",
    "    logging.warning(f\" {warning} | {id} | {desc}\")\n",
    "    \n",
    "    if id not in warningDF[\"id\"].values:\n",
    "        warningDF.loc[len(warningDF)] = {\"warning\": warning, \"id\": id, \"desc\": desc}\n",
    "        \n",
    "# -------------------------------------------------------------\n",
    "# SAFE_GET FUNCTION\n",
    "# -------------------------------------------------------------\n",
    "def safe_get(url, delay=(0, 3), retries=5):\n",
    "    for attempt in range(retries + 1):\n",
    "        # delay\n",
    "        if isinstance(delay, (int, float)):\n",
    "            time.sleep(delay)\n",
    "        else:\n",
    "            time.sleep(random.uniform(*delay))\n",
    "\n",
    "        try:\n",
    "            headers = {\n",
    "                \"User-Agent\": random.choice(USER_AGENTS),\n",
    "                \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "                \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "                \"Referer\": \"https://www.tfrrs.org/\",\n",
    "            }\n",
    "\n",
    "            response = session.get(url, headers=headers, timeout=40)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            if attempt == retries:\n",
    "                log_warning(warning=W_ATTEMPT_FAILED, id=url, desc=e)\n",
    "                return None\n",
    "            # brief backoff before retrying\n",
    "            sleep_time = min(5, (2 ** attempt)) + random.uniform(0, 1)\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# GET_TEAM_PAGE FUNCTION\n",
    "# -------------------------------------------------------------\n",
    "def get_team_page(url):\n",
    "    \"\"\"Return cached BeautifulSoup for a team page or None if fetch fails.\"\"\"\n",
    "    if url not in team_cache:\n",
    "        response = safe_get(url)\n",
    "        if response is None:\n",
    "            # Cache the failure so we don't retry\n",
    "            team_cache[url] = None\n",
    "        else:\n",
    "            team_cache[url] = BeautifulSoup(response.content, 'lxml')\n",
    "    return team_cache.get(url)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# NORMALIZE_NAME FUNCTION\n",
    "# -------------------------------------------------------------\n",
    "def normalize_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize athlete names for exact/fuzzy matching:\n",
    "    - Uppercase\n",
    "    - Remove accents\n",
    "    - Convert all whitespace to single space\n",
    "    - Strip leading/trailing spaces\n",
    "    - Remove non-letter characters except spaces\n",
    "    \"\"\"\n",
    "    if not name:\n",
    "        return \"\"\n",
    "\n",
    "    # Unicode normalize to separate accents\n",
    "    name = unicodedata.normalize(\"NFKD\", name)\n",
    "\n",
    "    # Remove accents\n",
    "    name = \"\".join([c for c in name if not unicodedata.combining(c)])\n",
    "\n",
    "    # Replace all weird spaces (thin, hair, non-breaking, zero-width) with normal space\n",
    "    name = re.sub(r\"[\\u00A0\\u2000-\\u200B\\u202F\\u205F\\u3000]\", \" \", name)\n",
    "\n",
    "    # Replace multiple spaces with single space\n",
    "    name = re.sub(r\"\\s+\", \" \", name)\n",
    "\n",
    "    # Strip leading/trailing spaces\n",
    "    name = name.strip()\n",
    "\n",
    "    # Remove punctuation except spaces\n",
    "    name = re.sub(r\"[^\\w\\s]\", \"\", name)\n",
    "\n",
    "    # Uppercase for case-insensitive comparison\n",
    "    name = name.upper()\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# TEAM_MAPPING FUNCTION\n",
    "# -------------------------------------------------------------\n",
    "def team_mapping(team):\n",
    "    if team == \"Elkhart Christian\":\n",
    "        return \"Elkhart Christian Academy\"\n",
    "    elif team == \"DeMotte Christian\":\n",
    "        return \"Covenant Christian DeMotte\"\n",
    "    elif team == \"LCA\":\n",
    "        return \"Lighthouse Christian Academy\"\n",
    "    else:\n",
    "        return team\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# GRADE_NORMALIZATION FUNCTION\n",
    "# -------------------------------------------------------------\n",
    "def grade_normalization(grade):\n",
    "\n",
    "    if grade in [\"Freshman\", \"Fr\", \"FR\", \"9\"]:\n",
    "        return \"FR\"\n",
    "    elif grade in [\"Sophomore\", \"So\", \"SO\", \"10\"]:\n",
    "        return \"SO\"\n",
    "    elif grade in [\"Junior\", \"Jr\", \"JR\", \"11\"]:\n",
    "        return \"JR\"\n",
    "    elif grade in [\"Senior\", \"Sr\", \"SR\", \"12\"]:\n",
    "        return \"SR\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# GET_OR_CREATE_ATHLETE\n",
    "# -------------------------------------------------------------\n",
    "def process_athlete(first, last, school_id, gender, grad_year):\n",
    "    first_key = (first or \"\").upper().strip()\n",
    "    last_key = (last or \"\").upper().strip()\n",
    "\n",
    "    key = (first_key, last_key, school_id, grad_year)\n",
    "\n",
    "    if key in athlete_db_cache:\n",
    "        return athlete_db_cache[key]\n",
    "\n",
    "    athlete_id = db.get_athlete_id(first_key, last_key, school_id, grad_year)\n",
    "    \n",
    "    if athlete_id is None:\n",
    "        athlete_id = db.insert_athlete(school_id, first_key, last_key, gender, grad_year, commit=False)\n",
    "\n",
    "    athlete_db_cache[key] = athlete_id\n",
    "    \n",
    "    return athlete_id\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# FUZZY_RATIO FUNCTION\n",
    "# -------------------------------------------------------------\n",
    "def fuzzy_ratio(a, b):\n",
    "    if not a or not b:\n",
    "        return 0\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# FUZZY_NAME_MATCH FUNCTION\n",
    "# -------------------------------------------------------------\n",
    "def fuzzy_name_match(name1, name2, threshold=0.85):\n",
    "    score = fuzzy_ratio(name1, name2)\n",
    "    return score >= threshold, score\n",
    "    \n",
    "# -------------------------------------------------------------\n",
    "# build_tfrrs_url\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def build_tfrrs_url(team, gender, suffix):\n",
    "    \"\"\"Build the TFRRS team URL.\"\"\"\n",
    "    team_clean = re.sub(r'[^a-zA-Z0-9_ -]', '', (team or \"\").strip())\n",
    "    team_clean = re.sub(r'\\sHS$', '', team_clean, flags=re.IGNORECASE)\n",
    "    team_clean = re.sub(r'\\s', '_', team_clean)\n",
    "    \n",
    "    url = f\"https://indiana.tfrrs.org/teams/tf/{team_clean}\"\n",
    "    url += \"_m.html\" if (gender or \"\").lower() in (\"boys\", \"m\", \"male\") else \"_f.html\"\n",
    "    url += suffix\n",
    "    \n",
    "    return url\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# parse_roster_table\n",
    "# -------------------------------------------------------------\n",
    "def parse_roster_table(team_soup):\n",
    "    \"\"\"Extract roster information from team page.\"\"\"\n",
    "    roster_header = team_soup.find(\"h3\", string=re.compile(\"ROSTER\", re.I))\n",
    "    roster_table = roster_header.find_next(\"table\") if roster_header else None\n",
    "    \n",
    "    if roster_table is None:\n",
    "        return None, []\n",
    "    \n",
    "    rows = roster_table.find_all(\"tr\")\n",
    "    roster_list = []\n",
    "    \n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) < 2:\n",
    "            continue\n",
    "        \n",
    "        name_text = cols[0].get_text(strip=True)\n",
    "        grade_text = cols[1].get_text(strip=True) if cols[1] else None\n",
    "        \n",
    "        if not name_text or not name_text.strip():\n",
    "            continue\n",
    "        \n",
    "        # Parse name\n",
    "        if \",\" in name_text:\n",
    "            r_last, r_first = map(str.strip, name_text.split(\",\", 1))\n",
    "        else:\n",
    "            parts = name_text.split()\n",
    "            r_first = parts[0]\n",
    "            r_last = \" \".join(parts[1:]) if len(parts) > 1 else \"\"\n",
    "        \n",
    "        roster_list.append((normalize_name(r_first), normalize_name(r_last), r_first, r_last, grade_text))\n",
    "    \n",
    "    return roster_table, roster_list\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# try_exact_match\n",
    "# -------------------------------------------------------------\n",
    "def try_exact_match(name_parts, roster_list):\n",
    "    \"\"\"Try all possible first/last splits for exact matching.\"\"\"\n",
    "    for split_index in range(1, len(name_parts)):\n",
    "        first_try = \" \".join(name_parts[:split_index])\n",
    "        last_try = \" \".join(name_parts[split_index:])\n",
    "        norm_first_try = normalize_name(first_try)\n",
    "        norm_last_try = normalize_name(last_try)\n",
    "        \n",
    "        for r_first_norm, r_last_norm, r_first, r_last, grade_text in roster_list:\n",
    "            if r_first_norm == norm_first_try and r_last_norm == norm_last_try:\n",
    "                return True, r_first, r_last, grade_text\n",
    "    \n",
    "    return False, \"\", \"\", None\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# try_fuzzy_match\n",
    "# -------------------------------------------------------------\n",
    "def try_fuzzy_match(name_parts, roster_list, full_name, url):\n",
    "    \"\"\"Try fuzzy matching as fallback.\"\"\"\n",
    "    norm_first = normalize_name(name_parts[0])\n",
    "    norm_last = normalize_name(\" \".join(name_parts[1:]))\n",
    "    \n",
    "    for r_first_norm, r_last_norm, r_first, r_last, grade_text in roster_list:\n",
    "        first_ok = fuzzy_ratio(r_first_norm, norm_first) >= 0.75\n",
    "        last_ok = fuzzy_ratio(r_last_norm, norm_last) >= 0.88\n",
    "        full_ok = fuzzy_ratio(r_first_norm + r_last_norm, norm_first + norm_last) >= 0.80\n",
    "        \n",
    "        if (first_ok and last_ok) or full_ok:\n",
    "            log_warning(\n",
    "                \"Fuzzy match used\",\n",
    "                id=f\"{full_name} <-> {r_first} {r_last}\",\n",
    "                desc=f\"{url}\"\n",
    "            )\n",
    "            return True, r_first, r_last, grade_text\n",
    "    \n",
    "    return False, \"\", \"\", None\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# try_top_marks_match\n",
    "# -------------------------------------------------------------\n",
    "def try_top_marks_match(team_soup, name_parts, full_name, url):\n",
    "    \"\"\"Try matching in Top Marks section as last resort.\"\"\"\n",
    "    headers = team_soup.find_all([\"h2\", \"h3\"])\n",
    "    top_marks_header = None\n",
    "    \n",
    "    for h in headers:\n",
    "        if \"top marks\" in h.get_text(\" \", strip=True).lower():\n",
    "            top_marks_header = h\n",
    "            break\n",
    "    \n",
    "    if not top_marks_header:\n",
    "        return False, \"\", \"\", None\n",
    "    \n",
    "    tm_table = top_marks_header.find_next(\"table\")\n",
    "    if not tm_table:\n",
    "        return False, \"\", \"\", None\n",
    "    \n",
    "    tm_rows = tm_table.find_all(\"tr\")\n",
    "    norm_first = normalize_name(name_parts[0])\n",
    "    norm_last = normalize_name(\" \".join(name_parts[1:]))\n",
    "    \n",
    "    for row in tm_rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        if len(cols) < 2:\n",
    "            continue\n",
    "        \n",
    "        name_text = cols[1].get_text(strip=True)\n",
    "        grade_text = cols[2].get_text(strip=True) if len(cols) > 2 else None\n",
    "        \n",
    "        if not name_text or not name_text.strip():\n",
    "            continue\n",
    "        \n",
    "        # Parse name\n",
    "        if \",\" in name_text:\n",
    "            tm_last, tm_first = map(str.strip, name_text.split(\",\", 1))\n",
    "        else:\n",
    "            parts = name_text.split()\n",
    "            tm_first = parts[0]\n",
    "            tm_last = \" \".join(parts[1:]) if len(parts) > 1 else \"\"\n",
    "        \n",
    "        tm_first_norm = normalize_name(tm_first)\n",
    "        tm_last_norm = normalize_name(tm_last)\n",
    "        \n",
    "        first_ok = fuzzy_ratio(tm_first_norm, norm_first) >= 0.75\n",
    "        last_ok = fuzzy_ratio(tm_last_norm, norm_last) >= 0.88\n",
    "        full_ok = fuzzy_ratio(tm_first_norm + tm_last_norm, norm_first + norm_last) >= 0.80\n",
    "        \n",
    "        if (first_ok and last_ok) or full_ok:\n",
    "            log_warning(\n",
    "                \"Top Marks used\",\n",
    "                id=f\"{full_name} <-> {tm_first} {tm_last}\",\n",
    "                desc=url\n",
    "            )\n",
    "            return True, tm_first, tm_last, grade_text\n",
    "    \n",
    "    return False, \"\", \"\", None\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# calculate_grad_year\n",
    "# -------------------------------------------------------------\n",
    "def calculate_grad_year(grade_text, year, full_name):\n",
    "    \"\"\"Calculate graduation year from grade level.\"\"\"\n",
    "    grade_text = grade_normalization(grade_text)\n",
    "    GRADE_TO_OFFSET = {\"SR\": 0, \"JR\": 1, \"SO\": 2, \"FR\": 3}\n",
    "    offset = GRADE_TO_OFFSET.get(grade_text, -1)\n",
    "    \n",
    "    if offset == -1:\n",
    "        log_warning(warning=W_GRADE_LEVEL_NOT_FOUND, id=full_name, desc=grade_text)\n",
    "        return 9999\n",
    "    \n",
    "    return year + offset\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# get_name FUNCTION \n",
    "# -------------------------------------------------------------\n",
    "def get_name(full_name, team, gender, year, suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Return standardized first, last, grad_year for an athlete.\n",
    "    - Exact match tries all possible first/last splits.\n",
    "    - Fuzzy match is fallback if exact match fails.\n",
    "    \"\"\"\n",
    "    team = team_mapping(team)\n",
    "    full_name = (full_name or \"\").strip()\n",
    "    name_parts = full_name.split()\n",
    "    \n",
    "    if len(name_parts) == 0:\n",
    "        return \"\", \"\", 9999\n",
    "    \n",
    "    # Build team URL\n",
    "    url = build_tfrrs_url(team, gender, suffix)\n",
    "    \n",
    "    # Get team page\n",
    "    team_soup = get_team_page(url)\n",
    "    if team_soup is None:\n",
    "        log_warning(warning=W_TEAM_NOT_FOUND, id=team, desc=url)\n",
    "        log_warning(warning=W_ATHLETE_NOT_FOUND, id=full_name, desc=url)\n",
    "        return full_name.upper(), full_name.upper(), 9999\n",
    "    \n",
    "    # Check cache\n",
    "    key = (url, full_name.upper())\n",
    "    if key in athlete_url_cache:\n",
    "        return athlete_url_cache[key]\n",
    "    \n",
    "    # Parse roster table\n",
    "    roster_table, roster_list = parse_roster_table(team_soup)\n",
    "    if roster_table is None:\n",
    "        log_warning(warning=W_ROSTER_NOT_FOUND, id=team, desc=url)\n",
    "        log_warning(warning=W_ATHLETE_NOT_FOUND, id=full_name, desc=url)\n",
    "        return full_name.upper(), full_name.upper(), 9999\n",
    "    \n",
    "    # Strategy 1: Try exact match\n",
    "    matched, roster_first, roster_last, match_grade_text = try_exact_match(name_parts, roster_list)\n",
    "    \n",
    "    # Strategy 2: Try fuzzy match (if exact failed)\n",
    "    if not matched:\n",
    "        matched, roster_first, roster_last, match_grade_text = try_fuzzy_match(\n",
    "            name_parts, roster_list, full_name, url\n",
    "        )\n",
    "    \n",
    "    # If matched on roster, calculate grad year and return\n",
    "    if matched:\n",
    "        grad_year = calculate_grad_year(match_grade_text, year, full_name)\n",
    "        result = (roster_first.upper(), roster_last.upper(), grad_year)\n",
    "        athlete_url_cache[key] = result\n",
    "        return result\n",
    "    \n",
    "    # Strategy 3: Try Top Marks section (last resort)\n",
    "    matched, roster_first, roster_last, match_grade_text = try_top_marks_match(\n",
    "        team_soup, name_parts, full_name, url\n",
    "    )\n",
    "    \n",
    "    # If matched in Top Marks, calculate grad year and return\n",
    "    if matched:\n",
    "        grad_year = calculate_grad_year(match_grade_text, year, full_name)\n",
    "        result = (roster_first.upper(), roster_last.upper(), grad_year)\n",
    "        athlete_url_cache[key] = result\n",
    "        return result\n",
    "    \n",
    "    # Still not found - return original name uppercased\n",
    "    log_warning(warning=W_ATHLETE_NOT_FOUND, id=full_name, desc=url)\n",
    "    result = (full_name.upper(), full_name.upper(), 9999)\n",
    "    athlete_url_cache[key] = result\n",
    "    return result\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# MAIN LOGIC - SCRAPING MEETS & EVENTS (build master_result_list)\n",
    "# -------------------------------------------------------------\n",
    "db = Database(DB_PATH)\n",
    "\n",
    "# insert tfrrs link info if needed (keeps your previous logic)\n",
    "try:\n",
    "    url = f'https://in.tfrrs.org/tournament.html?year={YEAR}'\n",
    "    response = safe_get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser') if response is not None else None\n",
    "except Exception:\n",
    "    soup = None\n",
    "    \n",
    "for meet_type in MEET_TYPE_LIST:\n",
    "    if soup is None:\n",
    "        break\n",
    "    if meet_type == \"Sectional\":\n",
    "        link1 = soup.find_all('a', href=True, string='Section 1')\n",
    "        link2 = soup.find_all('a', href=True, string='Section 2')\n",
    "    elif meet_type == \"Regional\":\n",
    "        link1 = soup.find_all('a', href=True, string='Region 1')\n",
    "        link2 = soup.find_all('a', href=True, string='Region 2')\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    if link1 and link2:\n",
    "        # keep original logic but guard indexing\n",
    "        try:\n",
    "            link_id1 = link1[0]['href'].split('/')[4] if GENDER == \"Girls\" else link1[1]['href'].split('/')[4]\n",
    "            link_id2 = link2[0]['href'].split('/')[4] if GENDER == \"Girls\" else link2[1]['href'].split('/')[4]\n",
    "            increment = abs(int(link_id1) - int(link_id2))\n",
    "            db.insert_tfrrs(YEAR, GENDER, meet_type, link_id1, increment, commit=False)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Could not parse link ids for {meet_type}: {e}\")\n",
    "\n",
    "# Initialize links/increments from DB (same logic as before)\n",
    "df_info = db.get_tfrrs_info(YEAR)\n",
    "info = ()\n",
    "for meet_type in MEET_TYPE_LIST:\n",
    "    girls_df = df_info[(df_info['gender'] == \"Girls\") & (df_info['meet_type'] == meet_type)]\n",
    "    boys_df = df_info[(df_info['gender'] == \"Boys\") & (df_info['meet_type'] == meet_type)]\n",
    "\n",
    "    girls_link_id = girls_df['link_id'].iloc[0] if not girls_df.empty else 0\n",
    "    boys_link_id = boys_df['link_id'].iloc[0] if not boys_df.empty else 0\n",
    "\n",
    "    girls_increment = girls_df['increment'].iloc[0] if not girls_df.empty else 1\n",
    "    boys_increment = boys_df['increment'].iloc[0] if not boys_df.empty else 1\n",
    "\n",
    "    num = 32 if meet_type == \"Sectional\" else 8 if meet_type == \"Regional\" else 1\n",
    "\n",
    "    info += ((meet_type, girls_link_id, boys_link_id, girls_increment, boys_increment, num),)\n",
    "\n",
    "master_result_list = []\n",
    "\n",
    "print(\"Starting meet processing.\")\n",
    "for tourny in info:\n",
    "    tourny_type, girls_link_id, boys_link_id, girls_increment, boys_increment, num_meets = tourny\n",
    "    id = girls_link_id if GENDER == \"Girls\" else boys_link_id\n",
    "    id_increment = girls_increment if GENDER == \"Girls\" else boys_increment\n",
    "\n",
    "    for i in range(num_meets):\n",
    "        if (tourny_type == \"Sectional\" and (i + 1) not in SEC_TO_PROCESS) or \\\n",
    "           (tourny_type == \"Regional\" and (i + 1) not in REG_TO_PROCESS):\n",
    "            id += id_increment\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"{GENDER} {tourny_type} {str(i + 1)}\")\n",
    "\n",
    "        base_url = f\"https://www.tfrrs.org/results/{id}\"\n",
    "        id += id_increment\n",
    "\n",
    "        base_url_response = safe_get(base_url)\n",
    "        if base_url_response is None:\n",
    "            logging.warning(f\"Failed to fetch base URL {base_url}\")\n",
    "            continue\n",
    "\n",
    "        base_soup = BeautifulSoup(base_url_response.text, 'lxml')\n",
    "\n",
    "        for event_info in events:\n",
    "            event_gender, event_type, event_name = event_info\n",
    "\n",
    "            link_tag = base_soup.find(\"a\", string=event_name)\n",
    "            if link_tag is None:\n",
    "                logging.warning(f\"Event '{event_name}' not found on page {base_url}\")\n",
    "                continue\n",
    "\n",
    "            event_url = link_tag.get(\"href\")\n",
    "            if not event_url:\n",
    "                logging.warning(f\"No href for event {event_name} on {base_url}\")\n",
    "                continue\n",
    "\n",
    "            if event_url.startswith(\"/\"):\n",
    "                event_url = \"https://www.tfrrs.org\" + event_url\n",
    "\n",
    "            response = safe_get(event_url)\n",
    "            if response is None:\n",
    "                logging.warning(f\"Failed to fetch event URL {event_url}\")\n",
    "                continue\n",
    "\n",
    "            event_soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "            # Extract hidden classes from <style>\n",
    "            all_styles = event_soup.find_all(\"style\")\n",
    "            ignore_classes = set()\n",
    "            for style in all_styles:\n",
    "                hidden = re.findall(r'\\.(round_\\d+_\\d+)\\s*{[^}]*display\\s*:\\s*none', style.text)\n",
    "                ignore_classes.update(hidden)\n",
    "\n",
    "            # Get meet info\n",
    "            meet_info = event_soup.find_all(\"div\", {\"class\": \"panel-heading-normal-text\"})\n",
    "            meet_host = meet_info[1].text.strip() if len(meet_info) > 1 else \"\"\n",
    "            meet_host = meet_host.split(\"-\")[0].strip() if \"-\" in meet_host else meet_host\n",
    "\n",
    "            # Get results table safely\n",
    "            tbodies = event_soup.find_all('tbody')\n",
    "            if event_type == \"Prelim\":\n",
    "                results = tbodies[1] if len(tbodies) > 1 else (tbodies[0] if tbodies else None)\n",
    "            else:\n",
    "                results = tbodies[0] if tbodies else None\n",
    "\n",
    "            if results is None:\n",
    "                logging.warning(f\"No results table found for {event_name} at {event_url}\")\n",
    "                response.close()\n",
    "                continue\n",
    "\n",
    "            rows = results.find_all('tr')\n",
    "\n",
    "            for r in rows:\n",
    "                cells = r.find_all('td')\n",
    "                if len(cells) <= 2:\n",
    "                    continue\n",
    "\n",
    "                visible_texts = [c.text.strip() for c in cells if not any(cls in ignore_classes for cls in c.get('class', []))]\n",
    "\n",
    "                result_list = [\n",
    "                    meet_host, tourny_type, i + 1, event_name, event_gender, event_type\n",
    "                ] + visible_texts\n",
    "\n",
    "                if event_type == \"Relay\":\n",
    "                    first, last = \"\", \"\"\n",
    "                else:\n",
    "                    # Attempt to get athlete name\n",
    "                    # result_list indices based on earlier layout; keep original mapping\n",
    "                    try:\n",
    "                        first, last, grad_year = get_name(result_list[7], result_list[9], result_list[4], YEAR, suffix=tfrrs_url_suffix)\n",
    "                    except Exception as e:\n",
    "                        logging.warning(f\"Error getting name for row: {e}\")\n",
    "                        first, last, grad_year = \"\", \"\", None\n",
    "\n",
    "                # standardize results into a consistent schema\n",
    "                try:\n",
    "                    if event_type == \"Relay\":\n",
    "                        result2 = convert.time_to_seconds(result_list[9])\n",
    "                        standardized_result_list = [*result_list[:9], '-', result_list[9], result_list[10], first, last, result2, '-']\n",
    "                    elif event_type == \"Prelim\":\n",
    "                        result2 = convert.time_to_seconds(result_list[10])\n",
    "                        standardized_result_list = [\n",
    "                            result_list[0], result_list[1], result_list[2], result_list[3],\n",
    "                            result_list[4], result_list[5], result_list[6], result_list[9],\n",
    "                            result_list[7], result_list[8], result_list[10], '-', first, last, result2, grad_year\n",
    "                        ]\n",
    "                    elif event_type == \"Field\":\n",
    "                        result2 = convert.distance_to_inches(result_list[10])\n",
    "                        standardized_result_list = [\n",
    "                            result_list[0], result_list[1], result_list[2], result_list[3],\n",
    "                            result_list[4], result_list[5], result_list[6], result_list[9],\n",
    "                            result_list[7], result_list[8], result_list[10], result_list[12],\n",
    "                            first, last, result2, grad_year\n",
    "                        ]\n",
    "                    else:\n",
    "                        result2 = convert.time_to_seconds(result_list[10])\n",
    "                        standardized_result_list = [\n",
    "                            result_list[0], result_list[1], result_list[2], result_list[3],\n",
    "                            result_list[4], result_list[5], result_list[6], result_list[9],\n",
    "                            result_list[7], result_list[8], result_list[10], result_list[11],\n",
    "                            first, last, result2, grad_year\n",
    "                        ]\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error standardizing result row: {e}\")\n",
    "                    continue\n",
    "\n",
    "                standardized_result_list[6] = standardized_result_list[6].replace(\"  \", \" \")\n",
    "                master_result_list.append(standardized_result_list)\n",
    "\n",
    "            response.close()\n",
    "        gc.collect()\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# CLEANUP SCRAPING\n",
    "# -------------------------------------------------------------\n",
    "session.close()\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# SAVE RESULTS TO CSV\n",
    "# -------------------------------------------------------------\n",
    "df = pd.DataFrame(master_result_list, columns=[\n",
    "    'Meet Host', 'Meet Type', 'Meet #', 'Event', 'Gender', 'Type',\n",
    "    'Place', 'Team', 'Name', 'Year', 'Result', 'Score', 'First',\n",
    "    'Last', 'Result2', 'GradYear'\n",
    "])\n",
    "\n",
    "meetDesc = MEET_TYPE_LIST[0] if len(MEET_TYPE_LIST) == 1 else \"Mix\"\n",
    "file = f\"{YEAR} {GENDER} {meetDesc} Results.csv\"\n",
    "df.to_csv(file, index=False)\n",
    "\n",
    "warningFile = f\"{YEAR} {GENDER} {meetDesc} Warnings.csv\"\n",
    "warningDF.to_csv(warningFile, index=False)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f\"End time:   {end_time.strftime('%H:%M:%S')}\")\n",
    "elapsed = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed}\")\n",
    "print(f\"{file} and {warningFile} created.\")\n",
    "print(\"Web scrapping completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e5d2a80b-11d0-4fbd-8643-6d57fd60d36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into DB...please wait.\n",
      "Start time: 20:31:45\n",
      "Inserting meets.\n",
      "Inserting individual athletes and results.\n",
      "Inserting relay athletes and results.\n",
      "End time:   20:31:58\n",
      "Elapsed time: 0:00:12.711962\n",
      "2025 Girls State Warnings.csv updated.\n",
      "Successfully completed and DB updated!!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------------------------------------\n",
    "# DB PROCESSING - load CSV into DB\n",
    "# -------------------------------------------------------------\n",
    "print(\"Loading data into DB...please wait.\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(f\"Start time: {start_time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Read CSV into DataFrame\n",
    "df = pd.read_csv(file, na_values=[\"\", \"NA\", \"N/A\"], keep_default_na=False)\n",
    "\n",
    "# load meets in db\n",
    "print(\"Inserting meets.\")\n",
    "meet_df = df[['Meet Host', 'Meet Type', 'Meet #', 'Gender']]\n",
    "meet_df = meet_df.drop_duplicates()\n",
    "\n",
    "for index, row in meet_df.iterrows():\n",
    "    meet_id = db.get_meet_id(row['Meet Type'], row['Meet #'], YEAR, row['Gender'])\n",
    "    if meet_id is None:\n",
    "        db.insert_meet(row['Meet Host'], row['Meet Type'], row['Meet #'], YEAR, row['Gender'], commit=False)\n",
    "\n",
    "# load athletes\n",
    "print(\"Inserting individual athletes and results.\")\n",
    "athlete_df = df[df['Type'] != 'Relay']\n",
    "\n",
    "# First pass: ensure athletes exist\n",
    "for index, row in athlete_df.iterrows():\n",
    "    team = row[\"Team\"]\n",
    "    school_id = db.get_school_id(team)\n",
    "    first = str(row[\"First\"])\n",
    "    last = str(row[\"Last\"])\n",
    "    gender = row[\"Gender\"]\n",
    "    grad_year = row[\"GradYear\"]\n",
    "    process_athlete(first, last, school_id, gender, grad_year)\n",
    "\n",
    "# Second pass: insert athlete results\n",
    "for index, row in athlete_df.iterrows():\n",
    "    school_id = db.get_school_id(row[\"Team\"])\n",
    "    first = row[\"First\"]\n",
    "    last = row[\"Last\"]\n",
    "    grad_year = row[\"GradYear\"]\n",
    "    \n",
    "    athlete_id = process_athlete(first, last, school_id, row[\"Gender\"], grad_year)\n",
    "\n",
    "    meet_type = row[\"Meet Type\"]\n",
    "    meet_num = row[\"Meet #\"]\n",
    "    gender = row[\"Gender\"]\n",
    "    meet_id = db.get_meet_id(meet_type, int(meet_num), int(YEAR), gender)\n",
    "\n",
    "    event = row[\"Event\"]\n",
    "    grade = row[\"Year\"]\n",
    "\n",
    "    grade = grade_normalization(grade)\n",
    "\n",
    "    result = row[\"Result\"]\n",
    "    result2 = row[\"Result2\"]\n",
    "    place = row[\"Place\"]\n",
    "\n",
    "    event_type = \"Prelim\" if row[\"Type\"] == \"Prelim\" else \"Final\"\n",
    "\n",
    "    athlete_result = db.get_athlete_result(athlete_id, meet_id, event, event_type)\n",
    "    if athlete_result is None:\n",
    "        db.insert_athlete_result(athlete_id, meet_id, event, event_type, grade, result, result2, place, commit=False)\n",
    "\n",
    "# Load relay results\n",
    "print(\"Inserting relay athletes and results.\")\n",
    "relay_df = df[df['Type'] == 'Relay']\n",
    "\n",
    "for index, row in relay_df.iterrows():\n",
    "    school_id = db.get_school_id(row[\"Team\"])\n",
    "    meet_type = row[\"Meet Type\"]\n",
    "    meet_num = row[\"Meet #\"]\n",
    "    gender = row[\"Gender\"]\n",
    "    meet_id = db.get_meet_id(meet_type, int(meet_num), int(YEAR), gender)\n",
    "\n",
    "    event = row[\"Event\"]\n",
    "    result = row[\"Result\"]\n",
    "    result2 = row[\"Result2\"]\n",
    "    place = row[\"Place\"]\n",
    "    athlete_names = row[\"Name\"]\n",
    "\n",
    "    relay_result = db.get_relay_result(school_id, meet_id, event)\n",
    "\n",
    "    if relay_result is None:\n",
    "        relay_id = db.insert_relay_result(school_id, meet_id, event, result, result2, place, athlete_names, commit=False)\n",
    "\n",
    "        if pd.isna(athlete_names):\n",
    "            log_warning(warning = W_RELAY_ATHLETES_NOT_FOUND, id = relay_id, desc = row[\"Team\"] + \" \" + event)\n",
    "            \n",
    "            # warning_msg = f\"Bad relay names {row['Team']} {event} {athlete_names}\"\n",
    "            # logging.warning(warning_msg)\n",
    "            # warningDF.loc[len(warningDF)] = {\"full_name\": row[\"Team\"] + \" \" + event, \"team_url\": athlete_names}\n",
    "            continue\n",
    "\n",
    "        for full_name in athlete_names.split(\",\"):\n",
    "            full_name = full_name.strip()\n",
    "            if not full_name:\n",
    "                continue\n",
    "            \n",
    "            first, last, grad_year = get_name(full_name, row[\"Team\"], gender, YEAR, suffix=tfrrs_url_suffix)\n",
    "            athlete_id = process_athlete(first, last, school_id, gender, grad_year)\n",
    "\n",
    "            try:\n",
    "                db.insert_relay_athlete(relay_id, athlete_id, commit=False)\n",
    "            except IntegrityError:\n",
    "                log_warning(warning = W_DUPLICATE_ATHLETE, id = (first + \" \" + last), desc = row[\"Team\"] + \" \" + event)\n",
    "\n",
    "# Save warnings (again) after DB processing\n",
    "warningDF.to_csv(warningFile, index=False)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f\"End time:   {end_time.strftime('%H:%M:%S')}\")\n",
    "elapsed = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed}\")\n",
    "\n",
    "db.do_commit()\n",
    "print(f\"{warningFile} updated.\")\n",
    "print(f\"Successfully completed and DB updated!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af0ca5a-e4b7-4018-8243-c5cb51c2d07f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
